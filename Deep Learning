# Deep Learning


from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
import keras
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import Model
from tensorflow.keras.models import Sequential
from PIL import Image
import numpy as np
from sklearn.preprocessing import OneHotEncoder
import os
import pathlib
import IPython.display as display
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd


train_images = []
train_labels = []

test_images = []
test_labels = []

count = 0
directory = '/content/gdrive/My Drive/Colab Notebooks'


# add in training positives
for img_path in os.listdir(directory + '/train/0/'):
  im = Image.open(directory + '/train/0/'+str(img_path))
  train_images.append(np.array(im))
  train_labels.append(0)
  count += 1
  if count > 100:
    break

# add in training negatives
count = 0
for img_path in os.listdir(directory + '/train/1/'):
  im = Image.open(directory + '/train/1/'+str(img_path))
  train_images.append(np.array(im))
  train_labels.append(1)
  count += 1
  if count > 100:
    break

train_labels = np.asarray(train_labels)
train_images = np.asarray(train_images)

train_images = train_images / 255

train_labels = np.reshape(train_labels, (-1,1))
enc = OneHotEncoder()
train_labels = enc.fit_transform(train_labels).toarray()


# add test positive images

count = 0
for img_path in os.listdir(directory + '/test/1/'):
  im = Image.open(directory + '/test/1/'+str(img_path))
  if(np.array(im).shape != (50, 50, 3)):
    continue
  test_images.append(np.array(im))
  test_labels.append(1)
  count += 1
  if count > 10:
    break

# add test negative images
count = 0
for img_path in os.listdir(directory + '/test/0/'):
  im = Image.open(directory + '/test/0/'+str(img_path))
  if(np.array(im).shape != (50, 50, 3)):
    continue
  test_images.append(np.array(im))
  test_labels.append(0)
  count += 1
  if count > 10:
    break

test_labels = np.asarray(test_labels)
test_images = np.asarray(test_images)

test_images = test_images / 255

test_labels_og = test_labels;
test_labels = np.reshape(test_labels, (-1,1))
enc = OneHotEncoder()
test_labels = enc.fit_transform(test_labels).toarray()


# new model

model = Sequential()
 
model.add(Conv2D(128, (3, 3), input_shape=(50,50,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.3))
 
model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.3))
 
model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.3))

model.add(Flatten())  
 
model.add(Dense(128))
model.add(Activation('relu'))
 
model.add(Dense(2))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy']
              )


model.fit(train_images, train_labels,
          batch_size=32,
          epochs=10)


predictions = []

count = 0
for img in test_images:
  img = np.expand_dims(img, axis=0)
  predictions.append(model.predict(img))

y_prob = model.predict(test_images) 
test_pred = y_prob.argmax(axis=-1)


cm = pd.DataFrame(confusion_matrix(test_labels_og, test_pred), columns=["T", "F"], index=["P", "N"])
print(cm)
print("Accuracy = %.2f%%" % ((cm.iloc[1, 1] + cm.iloc[0, 0]) / cm.values.sum() * 100))